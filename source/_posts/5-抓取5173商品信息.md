---
title: 5_抓取5173商品信息
date: 2018-02-24 21:44:26
tags:
---
今天这个爬虫真的是抓的我头都大飞了...
初步看这个网站的数据,结构清晰层次合理,完美啊!...
没想到有一个大坑坑坑坑坑...
  

<!--more-->

本来继上一篇blog所想,是学习redis分布式的,结果一天都卡在IP代理池的应用...
话不多说,首先确定我们需要的item结构,商品url,title,price,info,time,幸运的是5173网站商品结构清晰,从html中十分容易提取

![Image text](https://raw.githubusercontent.com/lranc/hexo_image/master/5-1.png)

可以看到,在商品列表中每一项商品栏可以提取item的前四项,而第五项,商品上架时间也可以通过提取的url访问新页面,再提取出去
![Image text](https://raw.githubusercontent.com/lranc/hexo_image/master/5-2.png)

这样计算大概需要369×40+369=15129次访问(14760个商品页+369个商品列表页),即可获取全部数据...
然而事与愿违,昨晚将基本代码完成之后,爬取了一页...爬虫便倒下了...
昨夜还想着,哇,终于遇到一个封IP的网站了,今天可以好好干一架,便喜滋滋地去睡觉了...

以下是总结
今天遇到的几个难点:
1).scrapy 使用proxy代理,完全没有用啊!!!肯定是我的使用方式不对!但是就是没找出原因,无论是百度还是知乎还是stackoverflow,都没有找到相应的解决方案,目前几个猜测点: 
	scrapy的版本问题,
	python3的问题
2).好,既然封IP,而自己用scrapy无法实现IP代理,那么就用scrapy.Request获取商品列表页,利用requests.get()获取商品详情页获取time,
问题又来了,在不设置time.sleep()的情况下,依然一下子就全封了
3).其实也是与上一项相关的,这就令人十分疑惑了,将近100个IP地址一下子全被封,是不是该网站反爬虫的方式不是IP,或者不止于IP?由于自己的知识掌握较少,在使用了随机UA之后,依然没能找出其封禁手段
4).没办法,只好在利用requests.get()的同时多sleep下了...目前爬虫仍在运行,不过14760个item却要4个多小时左右完成,确实速度太慢了,而且还不能确保这种抓取方式能否抓完...等待简直煎熬...

改进及构想:
1).利用redis分布式抓取,这也是这个项目原本最初的学习目的
2).可遍历商品列表,将369×40个商品的url,title,price,info字典格式保存至json,再在之后利用字典中的url增量抓取time,以避免抓取失败,失去进度,顺便进一步学习多线程、多进程、异步以及布隆过滤器,最后保存数据库

最后,希望抓取成功...
明天的任务是学习redis,以及mongo删除重复的数据

