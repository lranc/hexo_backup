---
title: 6_5173爬虫优化
date: 2018-02-25 14:49:25
tags:
---
今天继续前一天写的爬虫,昨晚在爬取到240页的时候,还是因为Ip地址不够而抓取失败了,今天的任务是在删除数据库中的重复数据之后增量抓取.
所以文章结构是:
1.删除MongoDB中的重复数据
2.布隆过滤器增量抓取
3.redis初步学习
  
<!--more-->

# 简单方法
在今天的爬取过程中发现,商品的发布时间其实在url里,所以并不需要访问其详情页进去爬取()
!!!错误
其商品url时间为商品上架时间,而不是商品价格时间的具体表现,而这个爬虫的目的是为了知晓商品的价格走势,故没用


# 1.删除MongoDB中的重复数据:
由于对MongoDB掌握不是很熟练,是在网上找到的方法.[1][]
[1]http://blog.csdn.net/cloume/article/details/74931998
该方法是先将数据导出为json,然后删除collection的数据,再新建唯一index,需要注意的是不能在删除重复数据后直接导入,虽然建立索引后插入数据,得到了一个每个url都唯一的collection,但是在之后的增量爬取时,因为index的升降序问题,导致有的数据无法插入,因此有2种方案(其实一样):
a.在建立索引后,导入数据,得到一份无重复的数据,再导出,再remove,增量爬取,最后插入第二次导出的数据
b.在建立索引后,导入数据,得到一份无重复的数据,新建一份collection,增量爬取,最后合并数据
具体代码:
```
$ mongoexport -d 5173_jw3_price -c jw3_goods -o jw3_goods.json
$ mongo
$ use 5173_jw3_price #切换数据库
$ db.jw3_goods.remove({}) #删除数据
$ db.jw3_goods.createIndex({goods_url:1},{unique:true}) #建立以goods_url唯一索引的文档
$ exit
# mongoimport -d 5173_jw3_price -c jw3_goods --upsert jw3_goods.json  #插入数据
```

# 2.布隆过滤器增量抓取:
对昨天的5173_spider.py文件进行修改,将以下部分以及逻辑判断加入
```
#首先将所有已抓取的goods_url导出:
$ mongoexport -d 5173_jw3_price -c jw3_goods -f goods_url -o jw3_goods_url.json
------
import json
import sys
from pybloom import ScalableBloomFilter #导入可动态扩展容量的ScalableBloomFilter

with open('jw3_goods_url.json','r') as f:
	url_list = [json.loads(x)['goods_url'] for x in f.readlines()]  #获取已抓取的url列表
sbf = ScalableBloomFilter(mode=ScalableBloomFilter.SMALL_SET_GROWTH)
for i in url_list:
	sbf.add(i)

print(sys.getsizeof(sbf))  #56
print(sys.getsizeof(url_list))  #61432
```

# 3.redis初步学习
见下一篇...