---
title: 2_剑网3_交易信息爬虫(待完善)
date: 2018-02-20 23:22:54
tags:
---
玩剑三许多年了,虽然已经AFK,但不时还是会接触到有关剑三的一些信息
以前作为一名小小的pvg玩家,进行过各种交易,遇到过各式各样的玩家,其中也出现过些许不如人意的体验.
由于剑三的交易金额往往较大,加之没有官方的交易系统,官方监管困难,其高额的收益与较低的成本导致交易的道德风险大大增加
因此想要建个网站,提供一些有关剑网3的交易信息.
  

<!--more-->
update : 2018-2-21
更改noises_id_list,从list更改为numpy.array()方式读取,节省内存

# 1.思路
1).遍历贴吧(由服吧再在之后爬取更多有关贴吧)(目前只爬取了乾坤一掷吧)
2).对贴吧首页(etc)的帖子标题及摘要,进行正则检索,出现触发词则对该帖首页进行严格触发词检索
3).对满足条件的帖子进行遍历爬取
4).继续遍历其他帖子
5).保存相关数据至数据库,以及增量爬取所需信息至本地文件
6).循环间断爬取增量内容
7).对爬取的帖子内容进行文本处理(停留于此)

# 2.具体
# 1).爬取的数据结构,即Items
```
class TiebaUrlItem(scrapy.Item):  #用于增量爬取
    cheater_url_dict = scrapy.Field()  #{url_id:[latest_page,latest_floor,reply_count]}

class TiebaNoisesItem(scrapy.Item):   #用于去重
	noises_id_list = scrapy.Field()   #[url_id1,url_id2,url_id3...etc]

class TiebaCheaterItem(scrapy.Item):
	cheater_url = scrapy.Field()  #贴子url
	url_id = scrapy.Field() #贴子id
	post_title = scrapy.Field()  #帖子标题
	floor_num = scrapy.Field()    #楼层数
	user_id = scrapy.Field()  #楼层id
	user_name=scrapy.Field()  #楼层姓名
	reply_content = scrapy.Field()  #举报内容
	image_url = scrapy.Field()   #举报内容图片
```

#2).spider主要结构(非完整代码)
```
class Tieba_CheaterSpider(CrawlSpider):
	pattern = re.compile(etc)
	start_urls = ['https://tieba.baidu.com/f?kw=%E4%B9%BE%E5%9D%A4%E4%B8%80%E6%8E%B7&ie=utf-8']
	rules=(
		Rule(LinkExtractor(allow=r'f?kw=%E4%B9%BE%E5%9D%A4%E4%B8%80%E6%8E%B7&ie=utf-8&pn=\d+',restrict_xpaths=('.//div[@id="frs_list_pager"]')),#
			callback='parse_index_url',follow=True),
		)
	with open('noises_id_list.txt','r') as f:
		noises_id_list = f.readlines()[0].split(',')  #去重列表
	new_noises=[]	#新噪点贴子id列表	
	try:
		with open('cheater_url_dict.json','r') as f:
			old_cheater_url_dict = json.load(f)
	except:
		old_cheater_url_dict = {}   #增量url_id字典
	def parse_index_url(self,response):	#处理贴吧页面,爬取举报贴
		bodys=
		for i in bodys:
			判断url_id是否是noises_id_list
			判断是否含有触发词 
			判断是否含有严格触发词 pri_judge
			判断是否需要增量爬取
			调用parse_all_detail
		yield TiebaUrlItem

	def parse_all_detail(): #爬取数据
		meta传递数据
		根据楼层信息与latest_floor对比, 大于则爬取,小于则不
		yield TiebaCheaterItem
		存在下一页 更新meta 并调用parse_all_detail()
		不存在 调取meta 保存至TiebaUrlItem
		yield TiebaUrlItem

	def pri_judge():
```

# 3).pipelines 以及 settings
a.pipelines 数据库处理,以及文件处理
b.settings 一些设置 log_level, 数据库参数, 去重及增量list,dict

# 4).存在的问题
a.下载中间件的使用,随机UA没有实现,在使用该中间件时,无法爬取网页
b.meta的使用
c.数据结构的理解

# 待增加的功能及 idea
由于技术问题,目前只做到贴吧帖子的抓取,对自动识别骗子,并提取相关信息没有想到好的解决办法
以下是一些展望
1).增加评论的抓取(似乎不是很重要,没有深入)
2).文本处理的完善,主动筛选出骗子信息(较难)
3).SMTP 发送邮件 (似乎不重要,较为简单)
4).查询后权限投票,增加查询者的便捷度
5).根据投票结果,对数据库进行删改,提高准确性
6).分布式优化,加快爬取速度

由于抓取了所有信息帖子信息,目前较为简单的做法是按照JX3YYYY.com作者的思路,由查询者提供关键词,返回相关信息(dict),由查询者自行判断是否是骗子
